{
    "examples": [
        {
            "query": "What are the potential risks associated with large language models (LLMs) according to the context information?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Evaluating Large Language Models: A\nComprehensive Survey\nZishan Guo\u2217, Renren Jin\u2217, Chuang Liu\u2217, Yufei Huang, Dan Shi, Supryadi\nLinhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong\u2020\nTianjin University\n{guozishan, rrjin, liuc_09, yuki_731, shidan, supryadi}@tju.edu.cn\n{linhaoyu, yan_liu, jiaxuanlee, xbj1355, dyxiong}@tju.edu.cn\nAbstract\nLarge language models (LLMs) have demonstrated remarkable capabilities\nacross a broad spectrum of tasks. They have attracted significant attention\nand been deployed in numerous downstream applications. Nevertheless, akin\nto a double-edged sword, LLMs also present potential risks. They could\nsuffer from private data leaks or yield inappropriate, harmful, or misleading\ncontent. Additionally, the rapid progress of LLMs raises concerns about the\npotential emergence of superintelligent systems without adequate safeguards.\nTo effectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\nThis survey endeavors to offer a panoramic perspective on the evaluation\nof LLMs. We categorize the evaluation of LLMs into three major groups:\nknowledgeandcapabilityevaluation, alignmentevaluationandsafetyevaluation.\nIn addition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs\u2019 performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\nWe hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making evaluation\nserve as a cornerstone in guiding the responsible development of LLMs. We\nenvision that this will channel their evolution into a direction that maximizes\nsocietal benefit while minimizing potential risks. A curated list of related\npapers has been publicly available at a GitHub repository.1\n\u2217Equal contribution\n\u2020Corresponding author.\n1https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers\n1arXiv:2310.19736v3  [cs.CL]  25 Nov 2023"
            ],
            "reference_answer": "According to the context information, the potential risks associated with large language models (LLMs) include the following:\n\n1. Private data leaks: LLMs have the potential to suffer from private data leaks, which can compromise the confidentiality and privacy of sensitive information.\n\n2. Inappropriate, harmful, or misleading content: LLMs can generate content that may be inappropriate, harmful, or misleading. This can have negative consequences in various applications, such as generating biased or offensive language.\n\n3. Emergence of superintelligent systems without safeguards: The rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems that lack adequate safeguards. This can lead to unforeseen and potentially dangerous behavior.\n\nTo ensure the safe and beneficial development of LLMs, it is crucial to conduct rigorous and comprehensive evaluations of their capabilities, alignment with human values, and safety measures.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "How does the survey categorize the evaluation of LLMs and what are the three major groups mentioned?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Evaluating Large Language Models: A\nComprehensive Survey\nZishan Guo\u2217, Renren Jin\u2217, Chuang Liu\u2217, Yufei Huang, Dan Shi, Supryadi\nLinhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong\u2020\nTianjin University\n{guozishan, rrjin, liuc_09, yuki_731, shidan, supryadi}@tju.edu.cn\n{linhaoyu, yan_liu, jiaxuanlee, xbj1355, dyxiong}@tju.edu.cn\nAbstract\nLarge language models (LLMs) have demonstrated remarkable capabilities\nacross a broad spectrum of tasks. They have attracted significant attention\nand been deployed in numerous downstream applications. Nevertheless, akin\nto a double-edged sword, LLMs also present potential risks. They could\nsuffer from private data leaks or yield inappropriate, harmful, or misleading\ncontent. Additionally, the rapid progress of LLMs raises concerns about the\npotential emergence of superintelligent systems without adequate safeguards.\nTo effectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\nThis survey endeavors to offer a panoramic perspective on the evaluation\nof LLMs. We categorize the evaluation of LLMs into three major groups:\nknowledgeandcapabilityevaluation, alignmentevaluationandsafetyevaluation.\nIn addition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs\u2019 performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\nWe hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making evaluation\nserve as a cornerstone in guiding the responsible development of LLMs. We\nenvision that this will channel their evolution into a direction that maximizes\nsocietal benefit while minimizing potential risks. A curated list of related\npapers has been publicly available at a GitHub repository.1\n\u2217Equal contribution\n\u2020Corresponding author.\n1https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers\n1arXiv:2310.19736v3  [cs.CL]  25 Nov 2023"
            ],
            "reference_answer": "The survey categorizes the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation, and safety evaluation.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What are the different types of reasoning discussed in the document?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Contents\n1 Introduction 4\n2 Taxonomy and Roadmap 6\n3 Knowledge and Capability Evaluation 8\n3.1 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2 Knowledge Completion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.3 Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.1 Commonsense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.2 Logical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.3.3 Multi-hop Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3.4 Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.4 Tool Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.4.1 Tool Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4.2 Tool Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4 Alignment Evaluation 21\n4.1 Ethics and Morality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.2.1 Societal Bias in Downstream Tasks . . . . . . . . . . . . . . . . . . . 24\n4.2.2 Societal Bias in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.3 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.3.1 Toxicity Identification and Classification . . . . . . . . . . . . . . . . 30\n4.3.2 Toxicity Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.4 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.4.1 Datasets for Evaluating Truthfulness . . . . . . . . . . . . . . . . . . 31\n4.4.2 Methods for Evaluating Truthfulness . . . . . . . . . . . . . . . . . . 34\n5 Safety Evaluation 37\n5.1 Robustness Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.1.1 Prompt Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.1.2 Task Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2"
            ],
            "reference_answer": "The different types of reasoning discussed in the document are:\n1. Commonsense Reasoning\n2. Logical Reasoning\n3. Multi-hop Reasoning\n4. Mathematical Reasoning",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "How is toxicity evaluated in language models according to the document?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Contents\n1 Introduction 4\n2 Taxonomy and Roadmap 6\n3 Knowledge and Capability Evaluation 8\n3.1 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2 Knowledge Completion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.3 Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.1 Commonsense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.2 Logical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.3.3 Multi-hop Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3.4 Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.4 Tool Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.4.1 Tool Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4.2 Tool Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4 Alignment Evaluation 21\n4.1 Ethics and Morality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.2.1 Societal Bias in Downstream Tasks . . . . . . . . . . . . . . . . . . . 24\n4.2.2 Societal Bias in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.3 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.3.1 Toxicity Identification and Classification . . . . . . . . . . . . . . . . 30\n4.3.2 Toxicity Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.4 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.4.1 Datasets for Evaluating Truthfulness . . . . . . . . . . . . . . . . . . 31\n4.4.2 Methods for Evaluating Truthfulness . . . . . . . . . . . . . . . . . . 34\n5 Safety Evaluation 37\n5.1 Robustness Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.1.1 Prompt Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.1.2 Task Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2"
            ],
            "reference_answer": "Toxicity is evaluated in language models according to the document through toxicity identification and classification. The document also mentions toxicity evaluation as a method for evaluating toxicity in language models.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "In the context of specialized LLMs evaluation, what are some specific fields mentioned in the document where LLMs are being evaluated?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "5.1.3 Alignment Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2 Risk Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2.1 Evaluating LLMs Behaviors . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2.2 Evaluating LLMs as Agents . . . . . . . . . . . . . . . . . . . . . . . 41\n6 Specialized LLMs Evaluation 42\n6.1 Biology and Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6.2 Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6.3 Legislation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n6.4 Computer Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.5 Finance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7 Evaluation Organization 46\n7.1 Benchmarks for NLU and NLG . . . . . . . . . . . . . . . . . . . . . . . . . 47\n7.2 Benchmarks for Knowledge and Reasoning . . . . . . . . . . . . . . . . . . . 48\n7.2.1 Evaluation Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.2.2 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n7.3 Benchmarks for Holistic Evaluation . . . . . . . . . . . . . . . . . . . . . . . 53\n7.3.1 Leaderboards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n7.3.2 Arena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n8 Future Directions 56\n8.1 Risk Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n8.2 Agent Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n8.3 Dynamic Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n8.4 Enhancement-Oriented Evaluation for LLMs . . . . . . . . . . . . . . . . . . 57\n9 Conclusion 57\n3"
            ],
            "reference_answer": "In the context of specialized LLMs evaluation, some specific fields mentioned in the document where LLMs are being evaluated are Biology and Medicine, Education, Legislation, Computer Science, and Finance.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What are the future directions mentioned in the document for evaluating LLMs?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "5.1.3 Alignment Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2 Risk Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2.1 Evaluating LLMs Behaviors . . . . . . . . . . . . . . . . . . . . . . . 39\n5.2.2 Evaluating LLMs as Agents . . . . . . . . . . . . . . . . . . . . . . . 41\n6 Specialized LLMs Evaluation 42\n6.1 Biology and Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6.2 Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6.3 Legislation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n6.4 Computer Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n6.5 Finance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7 Evaluation Organization 46\n7.1 Benchmarks for NLU and NLG . . . . . . . . . . . . . . . . . . . . . . . . . 47\n7.2 Benchmarks for Knowledge and Reasoning . . . . . . . . . . . . . . . . . . . 48\n7.2.1 Evaluation Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.2.2 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n7.3 Benchmarks for Holistic Evaluation . . . . . . . . . . . . . . . . . . . . . . . 53\n7.3.1 Leaderboards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n7.3.2 Arena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n8 Future Directions 56\n8.1 Risk Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n8.2 Agent Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n8.3 Dynamic Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n8.4 Enhancement-Oriented Evaluation for LLMs . . . . . . . . . . . . . . . . . . 57\n9 Conclusion 57\n3"
            ],
            "reference_answer": "The future directions mentioned in the document for evaluating LLMs are:\n\n1. Risk Evaluation\n2. Agent Evaluation\n3. Dynamic Evaluation\n4. Enhancement-Oriented Evaluation for LLMs",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What are some traditional benchmark tests that have been used to evaluate language models in natural language processing (NLP)?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "1 Introduction\nWhen we delve into the concept of intelligence, human intelligence naturally emerges as\nour benchmark. Over millennia, humanity has embarked on a continuous exploration of\nhuman intelligence, employing diverse methods for measurement and evaluation. This quest\nfor understanding intelligence encompasses an array of approaches, ranging from IQ tests\nand cognitive games to educational pursuits and professional accomplishments. Throughout\nhistory, our persistent efforts have been geared toward comprehending, assessing, and pushing\nthe boundaries of various facets of human intelligence.\nHowever, against the backdrop of the information age, a new dimension of intelligence is\nemerging, sparking widespread interest among scientists and researchers: machine intelligence.\nOne representative of this emerging field is language models in natural language processing\n(NLP). These language models, typically constructed using powerful deep neural networks,\npossess unprecedented language comprehension and generation capabilities. The question\nof how to measure and assess the level of this new type of intelligence has become a crucial\nissue.\nIn the nascent stages of NLP, researchers have commonly employed a set of straightforward\nbenchmark tests to evaluate their language models. These initial evaluations primarily\nconcentrate on aspects such as grammar and vocabulary, encompassing tasks like syntactic\nparsing, word sense disambiguation, and so on. In the early 1990s, the advent of the\nMUC evaluation (Grishman & Sundheim, 1996) has marked a significant milestone in the\nNLP community. The MUC evaluation primarily centers on information extraction tasks,\nchallenging participants to extract specific information from text. This evaluation framework\nplays a pivotal role in propelling the field of information extraction forward. Subsequently,\nwith the emergence of deep learning in the 2010s, the NLP community embraces more\nexpansive benchmarks like SNLI (Bowman et al., 2015) and SQuAD (Rajpurkar et al., 2016).\nThese benchmarks not only evaluate system performance but also provide ample data for\ntraining systems. They usually assign individual scores to models according to the adopted\nevaluation metrics, facilitating the measurement of task-specific accuracy.\nWith the emergence of large-scale pre-trained language models, exemplified by BERT (Devlin\net al., 2019), evaluation methods have gradually evolved to adapt to the performance\nassessment of these new types of general models. In response to this paradigm shift, the NLP\ncommunity has taken the initiative to orchestrate a myriad of shared tasks and challenges,\nincluding but not limited to SemEval (Nakov et al., 2019), CoNLL (Sang & Meulder, 2003),\nGLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a), and XNLI (Conneau et al.,\n2018). These endeavors entail aggregating scores for each model, offering a holistic measure\nof its overall performance. They have, in turn, fostered continuous refinement in NLP\nevaluation methodologies, creating a dynamic arena for researchers to compare and contrast\nthe capabilities of diverse systems.\nWith the continual expansion in the size of language models, large language models (LLMs)\nhave exhibited noteworthy performance under both zero- and few-shot settings, rivaling\nfine-tuned pre-trained models. This shift has precipitated a transformation in the evaluation\nlandscape, marking a departure from traditional task-centered benchmarks to a focus on\n4"
            ],
            "reference_answer": "Some traditional benchmark tests that have been used to evaluate language models in natural language processing (NLP) include syntactic parsing, word sense disambiguation, and the MUC evaluation for information extraction tasks.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "How has the evaluation landscape in NLP shifted with the emergence of large-scale pre-trained language models?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "1 Introduction\nWhen we delve into the concept of intelligence, human intelligence naturally emerges as\nour benchmark. Over millennia, humanity has embarked on a continuous exploration of\nhuman intelligence, employing diverse methods for measurement and evaluation. This quest\nfor understanding intelligence encompasses an array of approaches, ranging from IQ tests\nand cognitive games to educational pursuits and professional accomplishments. Throughout\nhistory, our persistent efforts have been geared toward comprehending, assessing, and pushing\nthe boundaries of various facets of human intelligence.\nHowever, against the backdrop of the information age, a new dimension of intelligence is\nemerging, sparking widespread interest among scientists and researchers: machine intelligence.\nOne representative of this emerging field is language models in natural language processing\n(NLP). These language models, typically constructed using powerful deep neural networks,\npossess unprecedented language comprehension and generation capabilities. The question\nof how to measure and assess the level of this new type of intelligence has become a crucial\nissue.\nIn the nascent stages of NLP, researchers have commonly employed a set of straightforward\nbenchmark tests to evaluate their language models. These initial evaluations primarily\nconcentrate on aspects such as grammar and vocabulary, encompassing tasks like syntactic\nparsing, word sense disambiguation, and so on. In the early 1990s, the advent of the\nMUC evaluation (Grishman & Sundheim, 1996) has marked a significant milestone in the\nNLP community. The MUC evaluation primarily centers on information extraction tasks,\nchallenging participants to extract specific information from text. This evaluation framework\nplays a pivotal role in propelling the field of information extraction forward. Subsequently,\nwith the emergence of deep learning in the 2010s, the NLP community embraces more\nexpansive benchmarks like SNLI (Bowman et al., 2015) and SQuAD (Rajpurkar et al., 2016).\nThese benchmarks not only evaluate system performance but also provide ample data for\ntraining systems. They usually assign individual scores to models according to the adopted\nevaluation metrics, facilitating the measurement of task-specific accuracy.\nWith the emergence of large-scale pre-trained language models, exemplified by BERT (Devlin\net al., 2019), evaluation methods have gradually evolved to adapt to the performance\nassessment of these new types of general models. In response to this paradigm shift, the NLP\ncommunity has taken the initiative to orchestrate a myriad of shared tasks and challenges,\nincluding but not limited to SemEval (Nakov et al., 2019), CoNLL (Sang & Meulder, 2003),\nGLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a), and XNLI (Conneau et al.,\n2018). These endeavors entail aggregating scores for each model, offering a holistic measure\nof its overall performance. They have, in turn, fostered continuous refinement in NLP\nevaluation methodologies, creating a dynamic arena for researchers to compare and contrast\nthe capabilities of diverse systems.\nWith the continual expansion in the size of language models, large language models (LLMs)\nhave exhibited noteworthy performance under both zero- and few-shot settings, rivaling\nfine-tuned pre-trained models. This shift has precipitated a transformation in the evaluation\nlandscape, marking a departure from traditional task-centered benchmarks to a focus on\n4"
            ],
            "reference_answer": "The evaluation landscape in NLP has shifted with the emergence of large-scale pre-trained language models. Previously, benchmark tests focused on specific tasks such as grammar and vocabulary. However, with the advent of large-scale pre-trained language models like BERT, evaluation methods have evolved to adapt to these general models. The NLP community has organized shared tasks and challenges, aggregating scores for each model to provide a holistic measure of its overall performance. This shift has led to a departure from traditional task-centered benchmarks to a focus on evaluating the performance of these new types of general models.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What are some potential risks associated with the deployment of advanced language models (LLMs) without thorough evaluation?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "capability-centered assessments. The demarcation lines among distinct downstream tasks\nhave begun to blur. In tandem with this trend, the landscape of evaluation benchmarks\ndesigned to appraise knowledge, reasoning, and various other capabilities has expanded. Many\nof these benchmarks are characterized by an abandonment of training data and are devised\nwith the overarching goal of providing a comprehensive evaluation of a model\u2019s capabilities\nunder zero- and few-shot settings (Hendrycks et al., 2021b, Zhong et al., 2023, Zhang et al.,\n2023b, Li et al., 2023e).\nThe rapid adoption of LLMs by the general public has been strikingly demonstrated by\nChatGPT (OpenAI, 2022), which amassed over 100 million users within just two months of\nits launch. This unprecedented growth underscores the transformative capabilities of these\nmodels, including natural text generation (Brown et al., 2020), code generation (Chen et al.,\n2021), and tool use (Nakano et al., 2021). However, alongside their promise, concerns have\nbeen raised about the potential risks if such capable models are deployed at scale without\nthorough and comprehensive evaluation. Critical issues such as perpetuating biases, spreading\nmisinformation, and compromising privacy need to be rigorously addressed. In response\nto these concerns, a dedicated line of research has emerged with a focus on empirically\nevaluating the extent to which LLMs align with human preferences and values. Whereas\nprevious studies have focused predominantly on capabilities, this strand of research aims to\nsteer the advancement and application of LLMs in ways that maximize their benefits while\nproactively mitigating risks.\nAdditionally, the burgeoning use of LLMs and their escalating integration into real-world\ncontexts underscore the profound impact that advanced AI systems and agents, underpinned\nby LLMs, are having on human society. Before these advanced AI systems are deployed, the\nsafety and reliability of LLMs must be prioritized. We provide a comprehensive exploration of\na series of safety issues related to LLMs such as robustness and disastrous risks. While these\nrisks may not be fully realized and appear at present, advanced LLMs have shown certain\ntendencies by revealing behaviors indicative of catastrophic risks and demonstrating abilities\nto perform higher-order tasks in current evaluations. Consequently, we believe that discussing\nof evaluating these risks is essential for guiding the future direction of safety research in\nLLMs.\nWhile numerous benchmarks have been developed to evaluate LLMs\u2019 capabilities and align-\nment with human values, these have often focused narrowly on performance within singular\ntasks or domains. To enable more comprehensive LLM assessment, this survey provides\na systematic literature review synthesizing efforts to evaluate these models across various\ndimensions. We summarize key points regarding general LLM benchmarks and evaluation\nmethodologies spanning knowledge, reasoning, tool learning, toxicity, truthfulness, robustness,\nand privacy.\nOur work significantly extends two recent surveys on LLM evaluation by Chang et al. (2023)\nand Liu et al. (2023i). While concurrent, our survey takes a distinct approach from these\nexisting reviews. Chang et al. (2023) structure their analysis around evaluation tasks, datasets,\nand methods. In contrast, our survey integrates insights across these categories to provide\na more holistic characterization of key advancements and limitations in LLM evaluation.\nAdditionally, Liu et al. (2023i) primarily focus their review on alignment evaluation for LLMs.\n5"
            ],
            "reference_answer": "Some potential risks associated with the deployment of advanced language models (LLMs) without thorough evaluation include perpetuating biases, spreading misinformation, and compromising privacy.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "How does this survey on LLM evaluation differ from previous reviews conducted by Chang et al. (2023) and Liu et al. (2023i)?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "capability-centered assessments. The demarcation lines among distinct downstream tasks\nhave begun to blur. In tandem with this trend, the landscape of evaluation benchmarks\ndesigned to appraise knowledge, reasoning, and various other capabilities has expanded. Many\nof these benchmarks are characterized by an abandonment of training data and are devised\nwith the overarching goal of providing a comprehensive evaluation of a model\u2019s capabilities\nunder zero- and few-shot settings (Hendrycks et al., 2021b, Zhong et al., 2023, Zhang et al.,\n2023b, Li et al., 2023e).\nThe rapid adoption of LLMs by the general public has been strikingly demonstrated by\nChatGPT (OpenAI, 2022), which amassed over 100 million users within just two months of\nits launch. This unprecedented growth underscores the transformative capabilities of these\nmodels, including natural text generation (Brown et al., 2020), code generation (Chen et al.,\n2021), and tool use (Nakano et al., 2021). However, alongside their promise, concerns have\nbeen raised about the potential risks if such capable models are deployed at scale without\nthorough and comprehensive evaluation. Critical issues such as perpetuating biases, spreading\nmisinformation, and compromising privacy need to be rigorously addressed. In response\nto these concerns, a dedicated line of research has emerged with a focus on empirically\nevaluating the extent to which LLMs align with human preferences and values. Whereas\nprevious studies have focused predominantly on capabilities, this strand of research aims to\nsteer the advancement and application of LLMs in ways that maximize their benefits while\nproactively mitigating risks.\nAdditionally, the burgeoning use of LLMs and their escalating integration into real-world\ncontexts underscore the profound impact that advanced AI systems and agents, underpinned\nby LLMs, are having on human society. Before these advanced AI systems are deployed, the\nsafety and reliability of LLMs must be prioritized. We provide a comprehensive exploration of\na series of safety issues related to LLMs such as robustness and disastrous risks. While these\nrisks may not be fully realized and appear at present, advanced LLMs have shown certain\ntendencies by revealing behaviors indicative of catastrophic risks and demonstrating abilities\nto perform higher-order tasks in current evaluations. Consequently, we believe that discussing\nof evaluating these risks is essential for guiding the future direction of safety research in\nLLMs.\nWhile numerous benchmarks have been developed to evaluate LLMs\u2019 capabilities and align-\nment with human values, these have often focused narrowly on performance within singular\ntasks or domains. To enable more comprehensive LLM assessment, this survey provides\na systematic literature review synthesizing efforts to evaluate these models across various\ndimensions. We summarize key points regarding general LLM benchmarks and evaluation\nmethodologies spanning knowledge, reasoning, tool learning, toxicity, truthfulness, robustness,\nand privacy.\nOur work significantly extends two recent surveys on LLM evaluation by Chang et al. (2023)\nand Liu et al. (2023i). While concurrent, our survey takes a distinct approach from these\nexisting reviews. Chang et al. (2023) structure their analysis around evaluation tasks, datasets,\nand methods. In contrast, our survey integrates insights across these categories to provide\na more holistic characterization of key advancements and limitations in LLM evaluation.\nAdditionally, Liu et al. (2023i) primarily focus their review on alignment evaluation for LLMs.\n5"
            ],
            "reference_answer": "This survey on LLM evaluation differs from previous reviews conducted by Chang et al. (2023) and Liu et al. (2023i) in several ways. \n\nFirst, Chang et al. (2023) structured their analysis around evaluation tasks, datasets, and methods, while this survey takes a distinct approach by integrating insights across these categories to provide a more holistic characterization of key advancements and limitations in LLM evaluation.\n\nSecond, Liu et al. (2023i) primarily focused their review on alignment evaluation for LLMs, whereas this survey covers a broader range of dimensions for evaluating LLMs, including knowledge, reasoning, tool learning, toxicity, truthfulness, robustness, and privacy.\n\nOverall, this survey aims to provide a more comprehensive assessment of LLMs by synthesizing efforts across various dimensions, going beyond the narrow focus of previous reviews.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the primary objective of the survey mentioned in the document?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Question \nAnsweringTool \nLearning\nReasoning\nKnowledge \nCompletionEthics \nand \nMorality Bias\nToxicity\nTruthfulnessRobustnessEvaluation\nRisk \nEvaluation\nBiology and \nMedicine\nEducationLegislationComputer \nScienceFinance\nBenchmarks for\nHolistic Evaluation\nBenchmarks \nforKnowledge and Reasoning\nBenchmarks \nforNLU and NLGKnowledge and Capability\nLarge Language \nModel EvaluationAlignment Evaluation\nSafety\nSpecialized LLMs\nEvaluation Organization\n\u2026Figure 1: Our proposed taxonomy of major categories and sub-categories of LLM evaluation.\nOur survey expands the scope to synthesize findings from both capability and alignment\nevaluations of LLMs. By complementing these previous surveys through an integrated\nperspective and expanded scope, our work provides a comprehensive overview of the current\nstate of LLM evaluation research. The distinctions between our survey and these two related\nworks further highlight the novel contributions of our study to the literature.\n2 Taxonomy and Roadmap\nThe primary objective of this survey is to meticulously categorize the evaluation of LLMs,\nfurnishing readers with a well-structured taxonomy framework. Through this framework,\nreaders can gain a nuanced understanding of LLMs\u2019 performance and the attendant challenges\nacross diverse and pivotal domains.\nNumerous studies posit that the bedrock of LLMs\u2019 capabilities resides in knowledge and\nreasoning, serving as the underpinning for their exceptional performance across a myriad of\ntasks. Nonetheless, the effective application of these capabilities necessitates a meticulous\nexamination of alignment concerns to ensure that the model\u2019s outputs remain consistent with\nuser expectations. Moreover, the vulnerability of LLMs to malicious exploits or inadvertent\nmisuse underscores the imperative nature of safety considerations. Once alignment and safety\nconcerns have been addressed, LLMs can be judiciously deployed within specialized domains,\ncatalyzing task automation and facilitating intelligent decision-making. Thus, our overarching\n6"
            ],
            "reference_answer": "The primary objective of the survey mentioned in the document is to meticulously categorize the evaluation of Large Language Models (LLMs) and provide readers with a well-structured taxonomy framework.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
     
        {
            "query": "Which conference is the paper \"Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics\" published in?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "on Logic Programming 2023 Workshops co-located with the 39th International Conference\non Logic Programming (ICLP 2023), London, United Kingdom, July 9th and 10th, 2023 ,\nvolume 3437 of CEUR Workshop Proceedings . CEUR-WS.org, 2023. URL https://\nceur-ws.org/Vol-3437/paper1LPLR.pdf .\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial NLI: A new benchmark for natural language understanding. In Dan Jurafsky,\nJoyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,\n2020, pp. 4885\u20134901. Association for Computational Linguistics, 2020. doi: 10.18653/V1/\n2020.ACL-MAIN.441. URL https://doi.org/10.18653/v1/2020.acl-main.441 .\nPawe\u0142 Niszczota and Sami Abbas. Gpt as a financial advisor. Available at SSRN 4384861 ,\n2023.\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz.\nCapabilities of GPT-4 on medical challenge problems. CoRR, abs/2303.13375, 2023. doi:\n10.48550/arXiv.2303.13375. URL https://doi.org/10.48550/arXiv.2303.13375 .\nNamkee Oh, Gyu-Seong Choi, and Woo Yong Lee. Chatgpt goes to the operating room:\nevaluating gpt-4 performance and its potential in surgical education and training in the era\nof large language models. Annals of Surgical Treatment and Research , 104(5):269, 2023.\nSantiago Onta\u00f1\u00f3n, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Logicinference: A\nnew dataset for teaching logical inference to seq2seq models. CoRR, abs/2203.15099, 2022.\ndoi: 10.48550/arXiv.2203.15099. URL https://doi.org/10.48550/arXiv.2203.15099 .\nOpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt/ , 2022.\nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.\n08774. URL https://doi.org/10.48550/arXiv.2303.08774 .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F.\nChristiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions\nwith human feedback. In NeurIPS , 2022. URL http://papers.nips.cc/paper_files/\npaper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html .\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. Understanding factuality in\nabstractive summarization with FRANK: A benchmark for factuality metrics. In Kristina\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven\nBethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of\nthe 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 ,\npp. 4812\u20134829. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.\nnaacl-main.383. URL https://doi.org/10.18653/v1/2021.naacl-main.383 .\n88"
            ],
            "reference_answer": "The paper \"Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics\" is published in the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) 2021.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "In the paper \"Cross-lingual name tagging and linking for 282 languages,\" what is the main focus of the research conducted by Xiaoman Pan et al.?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji.\nCross-lingual name tagging and linking for 282 languages. In Regina Barzilay and Min-Yen\nKan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers , pp.\n1946\u20131958. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1178.\nURL https://doi.org/10.18653/v1/P17-1178 .\nZachary A. Pardos and Shreya Bhandari. Learning gain differences between chatgpt and\nhuman tutor generated algebra hints. CoRR, abs/2302.06871, 2023. doi: 10.48550/arXiv.\n2302.06871. URL https://doi.org/10.48550/arXiv.2302.06871 .\nAaron Parisi, Yao Zhao, and Noah Fiedel. TALM: tool augmented language models. CoRR,\nabs/2205.12255, 2022. doi: 10.48550/arXiv.2205.12255. URL https://doi.org/10.48550/\narXiv.2205.12255 .\nJiHoPark, JaminShin, andPascaleFung. Reducinggenderbiasinabusivelanguagedetection.\nIn Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018 , pp. 2799\u20132804. Association for Computational\nLinguistics, 2018. doi: 10.18653/v1/d18-1302. URL https://doi.org/10.18653/v1/\nd18-1302 .\nSan-Hee Park, Kang-Min Kim, O-Joun Lee, Youjin Kang, Jaewon Lee, Su-Min Lee, and\nSangKeun Lee. \"why do I feel offended?\" - korean dataset for offensive language iden-\ntification. In Andreas Vlachos and Isabelle Augenstein (eds.), Findings of the As-\nsociation for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6,\n2023, pp. 1112\u20131123. Association for Computational Linguistics, 2023. URL https:\n//aclanthology.org/2023.findings-eacl.85 .\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana\nThompson, Phu Mon Htut, and Samuel R. Bowman. BBQ: A hand-built bias benchmark\nfor question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio\n(eds.),Findings of the Association for Computational Linguistics: ACL 2022, Dublin,\nIreland, May 22-27, 2022 , pp. 2086\u20132105. Association for Computational Linguistics,\n2022. doi: 10.18653/v1/2022.findings-acl.165. URL https://doi.org/10.18653/v1/\n2022.findings-acl.165 .\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured\ntables. In Proceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language Processing of\nthe Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing,\nChina, Volume 1: Long Papers , pp. 1470\u20131480. The Association for Computer Linguistics,\n2015. doi: 10.3115/V1/P15-1142. URL https://doi.org/10.3115/v1/p15-1142 .\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve\nsimple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,\nDilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,\n89"
            ],
            "reference_answer": "The main focus of the research conducted by Xiaoman Pan et al. in the paper \"Cross-lingual name tagging and linking for 282 languages\" is cross-lingual name tagging and linking for a large number of languages.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "In the paper \"Reducing gender bias in abusive language detection,\" what approach was proposed by JiHo Park et al. to address the issue of gender bias?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji.\nCross-lingual name tagging and linking for 282 languages. In Regina Barzilay and Min-Yen\nKan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers , pp.\n1946\u20131958. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1178.\nURL https://doi.org/10.18653/v1/P17-1178 .\nZachary A. Pardos and Shreya Bhandari. Learning gain differences between chatgpt and\nhuman tutor generated algebra hints. CoRR, abs/2302.06871, 2023. doi: 10.48550/arXiv.\n2302.06871. URL https://doi.org/10.48550/arXiv.2302.06871 .\nAaron Parisi, Yao Zhao, and Noah Fiedel. TALM: tool augmented language models. CoRR,\nabs/2205.12255, 2022. doi: 10.48550/arXiv.2205.12255. URL https://doi.org/10.48550/\narXiv.2205.12255 .\nJiHoPark, JaminShin, andPascaleFung. Reducinggenderbiasinabusivelanguagedetection.\nIn Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.), Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018 , pp. 2799\u20132804. Association for Computational\nLinguistics, 2018. doi: 10.18653/v1/d18-1302. URL https://doi.org/10.18653/v1/\nd18-1302 .\nSan-Hee Park, Kang-Min Kim, O-Joun Lee, Youjin Kang, Jaewon Lee, Su-Min Lee, and\nSangKeun Lee. \"why do I feel offended?\" - korean dataset for offensive language iden-\ntification. In Andreas Vlachos and Isabelle Augenstein (eds.), Findings of the As-\nsociation for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6,\n2023, pp. 1112\u20131123. Association for Computational Linguistics, 2023. URL https:\n//aclanthology.org/2023.findings-eacl.85 .\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana\nThompson, Phu Mon Htut, and Samuel R. Bowman. BBQ: A hand-built bias benchmark\nfor question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio\n(eds.),Findings of the Association for Computational Linguistics: ACL 2022, Dublin,\nIreland, May 22-27, 2022 , pp. 2086\u20132105. Association for Computational Linguistics,\n2022. doi: 10.18653/v1/2022.findings-acl.165. URL https://doi.org/10.18653/v1/\n2022.findings-acl.165 .\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured\ntables. In Proceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language Processing of\nthe Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing,\nChina, Volume 1: Long Papers , pp. 1470\u20131480. The Association for Computer Linguistics,\n2015. doi: 10.3115/V1/P15-1142. URL https://doi.org/10.3115/v1/p15-1142 .\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve\nsimple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,\nDilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,\n89"
            ],
            "reference_answer": "The context information does not provide any details about the approach proposed by JiHo Park et al. in the paper \"Reducing gender bias in abusive language detection.\"",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the title of the paper mentioned in the context information and who are the editors of the proceedings it is published in?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pp. 2080\u20132094. Association for Computational\nLinguistics, 2021. doi: 10.18653/v1/2021.naacl-main.168. URL https://doi.org/10.\n18653/v1/2021.naacl-main.168 .\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language\nmodel connected with massive apis. CoRR, abs/2305.15334, 2023. doi: 10.48550/ARXIV.\n2305.15334. URL https://doi.org/10.48550/arXiv.2305.15334 .\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors\nfor word representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.),\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL , pp. 1532\u20131543. ACL, 2014. doi: 10.3115/v1/d14-1162. URL\nhttps://doi.org/10.3115/v1/d14-1162 .\nEthan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner,\nCraig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna\nChen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher\nOlah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson,\nGuro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon\nHyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas,\nMichael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph,\nNoem\u00ed Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott\nJohnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton,\nTom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark,\nSamuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan\nHubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors with\nmodel-written evaluations. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki\n(eds.),Findings of the Association for Computational Linguistics: ACL 2023, Toronto,\nCanada, July 9-14, 2023 , pp. 13387\u201313434. Association for Computational Linguistics,\n2023. doi: 10.18653/v1/2023.findings-acl.847. URL https://doi.org/10.18653/v1/2023.\nfindings-acl.847 .\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Marilyn A.\nWalker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume\n1 (Long Papers) , pp. 2227\u20132237. Association for Computational Linguistics, 2018. doi:\n10.18653/v1/n18-1202. URL https://doi.org/10.18653/v1/n18-1202 .\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander H. Miller. Language models as knowledge bases? In Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference\n90"
            ],
            "reference_answer": "The title of the paper mentioned in the context information is \"Discovering language model behaviors with model-written evaluations.\" The editors of the proceedings it is published in are Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "Which language model is described in the context information and what is its purpose?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pp. 2080\u20132094. Association for Computational\nLinguistics, 2021. doi: 10.18653/v1/2021.naacl-main.168. URL https://doi.org/10.\n18653/v1/2021.naacl-main.168 .\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language\nmodel connected with massive apis. CoRR, abs/2305.15334, 2023. doi: 10.48550/ARXIV.\n2305.15334. URL https://doi.org/10.48550/arXiv.2305.15334 .\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors\nfor word representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.),\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL , pp. 1532\u20131543. ACL, 2014. doi: 10.3115/v1/d14-1162. URL\nhttps://doi.org/10.3115/v1/d14-1162 .\nEthan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner,\nCraig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna\nChen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher\nOlah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson,\nGuro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon\nHyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas,\nMichael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph,\nNoem\u00ed Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott\nJohnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton,\nTom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark,\nSamuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan\nHubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors with\nmodel-written evaluations. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki\n(eds.),Findings of the Association for Computational Linguistics: ACL 2023, Toronto,\nCanada, July 9-14, 2023 , pp. 13387\u201313434. Association for Computational Linguistics,\n2023. doi: 10.18653/v1/2023.findings-acl.847. URL https://doi.org/10.18653/v1/2023.\nfindings-acl.847 .\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Marilyn A.\nWalker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume\n1 (Long Papers) , pp. 2227\u20132237. Association for Computational Linguistics, 2018. doi:\n10.18653/v1/n18-1202. URL https://doi.org/10.18653/v1/n18-1202 .\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander H. Miller. Language models as knowledge bases? In Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference\n90"
            ],
            "reference_answer": "The language model described in the context information is \"Gorilla: Large language model connected with massive APIs.\" Its purpose is not explicitly mentioned in the given context.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the main focus of the paper \"CREATOR: disentangling abstract and concrete reasonings of large language models through tool creation\" by Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019 , pp. 2463\u20132473. Association for Computational Linguistics, 2019. doi:\n10.18653/v1/D19-1250. URL https://doi.org/10.18653/v1/D19-1250 .\nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR:\ndisentangling abstract and concrete reasonings of large language models through tool\ncreation. CoRR, abs/2305.14318, 2023. doi: 10.48550/arXiv.2305.14318. URL https:\n//doi.org/10.48550/arXiv.2305.14318 .\nShuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. Making language models\nbetter tool learners with execution feedback. CoRR, abs/2305.13068, 2023. doi: 10.48550/\narXiv.2305.13068. URL https://doi.org/10.48550/arXiv.2305.13068 .\nLianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui.\nTIMEDIAL: temporal commonsense reasoning in dialog. In Chengqing Zong, Fei Xia,\nWenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\nAugust 1-6, 2021 , pp. 7066\u20137076. Association for Computational Linguistics, 2021. doi: 10.\n18653/v1/2021.acl-long.549. URL https://doi.org/10.18653/v1/2021.acl-long.549 .\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han,\nNing Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and\nJie Zhou. Webcpm: Interactive web search for chinese long-form question answering.\nIn Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pp. 8968\u20138988. Association\nfor Computational Linguistics, 2023a. doi: 10.18653/v1/2023.acl-long.499. URL https:\n//doi.org/10.18653/v1/2023.acl-long.499 .\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\nHuang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian,\nRunchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye,\nBowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning\nwith foundation models. CoRR, abs/2304.08354, 2023b. doi: 10.48550/arXiv.2304.08354.\nURL https://doi.org/10.48550/arXiv.2304.08354 ."
            ],
            "reference_answer": "The main focus of the paper \"CREATOR: disentangling abstract and concrete reasonings of large language models through tool creation\" is to explore the disentanglement of abstract and concrete reasonings in large language models through the creation of tools.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "How does the paper \"TIMEDIAL: temporal commonsense reasoning in dialog\" by Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui contribute to the field of natural language processing?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019 , pp. 2463\u20132473. Association for Computational Linguistics, 2019. doi:\n10.18653/v1/D19-1250. URL https://doi.org/10.18653/v1/D19-1250 .\nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR:\ndisentangling abstract and concrete reasonings of large language models through tool\ncreation. CoRR, abs/2305.14318, 2023. doi: 10.48550/arXiv.2305.14318. URL https:\n//doi.org/10.48550/arXiv.2305.14318 .\nShuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. Making language models\nbetter tool learners with execution feedback. CoRR, abs/2305.13068, 2023. doi: 10.48550/\narXiv.2305.13068. URL https://doi.org/10.48550/arXiv.2305.13068 .\nLianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui.\nTIMEDIAL: temporal commonsense reasoning in dialog. In Chengqing Zong, Fei Xia,\nWenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\nAugust 1-6, 2021 , pp. 7066\u20137076. Association for Computational Linguistics, 2021. doi: 10.\n18653/v1/2021.acl-long.549. URL https://doi.org/10.18653/v1/2021.acl-long.549 .\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han,\nNing Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and\nJie Zhou. Webcpm: Interactive web search for chinese long-form question answering.\nIn Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pp. 8968\u20138988. Association\nfor Computational Linguistics, 2023a. doi: 10.18653/v1/2023.acl-long.499. URL https:\n//doi.org/10.18653/v1/2023.acl-long.499 .\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\nHuang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian,\nRunchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye,\nBowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning\nwith foundation models. CoRR, abs/2304.08354, 2023b. doi: 10.48550/arXiv.2304.08354.\nURL https://doi.org/10.48550/arXiv.2304.08354 ."
            ],
            "reference_answer": "The paper \"TIMEDIAL: temporal commonsense reasoning in dialog\" by Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui contributes to the field of natural language processing by exploring temporal commonsense reasoning in dialog. It presents a novel approach to incorporating temporal reasoning into dialog systems, which is an important aspect of natural language understanding. The paper's findings and techniques can potentially improve the performance and capabilities of dialog systems in understanding and generating responses that involve temporal aspects.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
       
        {
            "query": "What is the title of the paper mentioned in the context information?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "2023\u20132038. Association for Computational Linguistics,\n2022. doi: 10.18653/v1/2022.emnlp-main.131. URL https://doi.org/10.18653/v1/\n2022.emnlp-main.131 .\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning. CoRR, abs/1709.00103, 2017. URL\nhttp://arxiv.org/abs/1709.00103 .\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin\nSaied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating\nfoundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL\nhttps://doi.org/10.48550/arXiv.2304.06364 .\n109"
            ],
            "reference_answer": "The title of the paper mentioned in the context information is \"Seq2sql: Generating structured queries from natural language using reinforcement learning\".",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the purpose of the Agieval benchmark mentioned in the context information?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "2023\u20132038. Association for Computational Linguistics,\n2022. doi: 10.18653/v1/2022.emnlp-main.131. URL https://doi.org/10.18653/v1/\n2022.emnlp-main.131 .\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning. CoRR, abs/1709.00103, 2017. URL\nhttp://arxiv.org/abs/1709.00103 .\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin\nSaied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating\nfoundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL\nhttps://doi.org/10.48550/arXiv.2304.06364 .\n109"
            ],
            "reference_answer": "The purpose of the Agieval benchmark mentioned in the context information is to evaluate foundation models in a human-centric manner.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the title of the paper authored by Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. \"going on a vacation\" takes longer\nthan \"going for a walk\": A study of temporal commonsense understanding. In Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019 , pp. 3361\u20133367. Association for Computational Linguistics, 2019. doi:\n10.18653/v1/D19-1332. URL https://doi.org/10.18653/v1/D19-1332 .\nBen Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth.\nTemporal reasoning on implicit events from distant supervision. In Kristina Toutanova,\nAnna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard,\nRyan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pp. 1361\u20131371.\nAssociation for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.107.\nURL https://doi.org/10.18653/v1/2021.naacl-main.107 .\nJingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang, Qun\nLiu, andHelenMeng. Towardsidentifyingsocialbiasindialogsystems: Frame, datasets, and\nbenchmarks. CoRR, abs/2202.08011, 2022. URL https://arxiv.org/abs/2202.08011 .\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi\nCheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic\nweb environment for building autonomous agents. CoRR, abs/2307.13854, 2023. doi:\n10.48550/arXiv.2307.13854. URL https://doi.org/10.48550/arXiv.2307.13854 .\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang,\nWei Ye, Neil Zhenqiang Gong, Yue Zhang, and Xing Xie. Promptbench: Towards evaluating\nthe robustness of large language models on adversarial prompts. CoRR, abs/2306.04528,\n2023a. doi: 10.48550/arXiv.2306.04528. URL https://doi.org/10.48550/arXiv.2306.\n04528.\nYiming Zhu, Peixian Zhang, Ehsan ul Haq, Pan Hui, and Gareth Tyson. Can chatgpt repro-\nduce human-generated labels? A study of social computing tasks. CoRR, abs/2304.10145,\n2023b. doi: 10.48550/ARXIV.2304.10145. URL https://doi.org/10.48550/arXiv.2304.\n10145.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset\nfor LLM question answering with external tools. CoRR, abs/2306.13304, 2023. doi:\n10.48550/arXiv.2306.13304. URL https://doi.org/10.48550/arXiv.2306.13304 .\nCaleb Ziems, Jane A. Yu, Yi-Chia Wang, Alon Y. Halevy, and Diyi Yang. The moral integrity\ncorpus: A benchmark for ethical dialogue systems. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio (eds."
            ],
            "reference_answer": "The title of the paper authored by Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth is \"going on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "In which year was the paper \"going on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding\" published?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. \"going on a vacation\" takes longer\nthan \"going for a walk\": A study of temporal commonsense understanding. In Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019 , pp. 3361\u20133367. Association for Computational Linguistics, 2019. doi:\n10.18653/v1/D19-1332. URL https://doi.org/10.18653/v1/D19-1332 .\nBen Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth.\nTemporal reasoning on implicit events from distant supervision. In Kristina Toutanova,\nAnna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard,\nRyan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pp. 1361\u20131371.\nAssociation for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.107.\nURL https://doi.org/10.18653/v1/2021.naacl-main.107 .\nJingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang, Qun\nLiu, andHelenMeng. Towardsidentifyingsocialbiasindialogsystems: Frame, datasets, and\nbenchmarks. CoRR, abs/2202.08011, 2022. URL https://arxiv.org/abs/2202.08011 .\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi\nCheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic\nweb environment for building autonomous agents. CoRR, abs/2307.13854, 2023. doi:\n10.48550/arXiv.2307.13854. URL https://doi.org/10.48550/arXiv.2307.13854 .\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang,\nWei Ye, Neil Zhenqiang Gong, Yue Zhang, and Xing Xie. Promptbench: Towards evaluating\nthe robustness of large language models on adversarial prompts. CoRR, abs/2306.04528,\n2023a. doi: 10.48550/arXiv.2306.04528. URL https://doi.org/10.48550/arXiv.2306.\n04528.\nYiming Zhu, Peixian Zhang, Ehsan ul Haq, Pan Hui, and Gareth Tyson. Can chatgpt repro-\nduce human-generated labels? A study of social computing tasks. CoRR, abs/2304.10145,\n2023b. doi: 10.48550/ARXIV.2304.10145. URL https://doi.org/10.48550/arXiv.2304.\n10145.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset\nfor LLM question answering with external tools. CoRR, abs/2306.13304, 2023. doi:\n10.48550/arXiv.2306.13304. URL https://doi.org/10.48550/arXiv.2306.13304 .\nCaleb Ziems, Jane A. Yu, Yi-Chia Wang, Alon Y. Halevy, and Diyi Yang. The moral integrity\ncorpus: A benchmark for ethical dialogue systems. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio (eds."
            ],
            "reference_answer": "The paper \"going on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding was published in 2019.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the file type of the document \"2310.19736.pdf\"?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "doi: 10.48550/ARXIV.2304.10145. URL https://doi.org/10.48550/arXiv.2304.\n10145.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset\nfor LLM question answering with external tools. CoRR, abs/2306.13304, 2023. doi:\n10.48550/arXiv.2306.13304. URL https://doi.org/10.48550/arXiv.2306.13304 .\nCaleb Ziems, Jane A. Yu, Yi-Chia Wang, Alon Y. Halevy, and Diyi Yang. The moral integrity\ncorpus: A benchmark for ethical dialogue systems. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022 , pp. 3755\u20133773. Association for Computational Linguistics, 2022. doi: 10.\n18653/v1/2022.acl-long.261. URL https://doi.org/10.18653/v1/2022.acl-long.261 .\n110"
            ],
            "reference_answer": "The file type of the document \"2310.19736.pdf\" is application/pdf.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "Who are the authors of the paper titled \"The moral integrity corpus: A benchmark for ethical dialogue systems\"?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "doi: 10.48550/ARXIV.2304.10145. URL https://doi.org/10.48550/arXiv.2304.\n10145.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset\nfor LLM question answering with external tools. CoRR, abs/2306.13304, 2023. doi:\n10.48550/arXiv.2306.13304. URL https://doi.org/10.48550/arXiv.2306.13304 .\nCaleb Ziems, Jane A. Yu, Yi-Chia Wang, Alon Y. Halevy, and Diyi Yang. The moral integrity\ncorpus: A benchmark for ethical dialogue systems. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022 , pp. 3755\u20133773. Association for Computational Linguistics, 2022. doi: 10.\n18653/v1/2022.acl-long.261. URL https://doi.org/10.18653/v1/2022.acl-long.261 .\n110"
            ],
            "reference_answer": "The authors of the paper titled \"The moral integrity corpus: A benchmark for ethical dialogue systems\" are Caleb Ziems, Jane A. Yu, Yi-Chia Wang, Alon Y. Halevy, and Diyi Yang.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the title of the paper mentioned in the context information?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Nur Bengisu \u00c7am and Arzucan \u00d6zg\u00fcr. Evaluation of chatgpt and bert-based models for\nturkish hate speech detection. In 2023 8th International Conference on Computer Science\nand Engineering (UBMK) , pp. 229\u2013233, 2023. doi: 10.1109/UBMK59864.2023.10286663.\n111"
            ],
            "reference_answer": "The title of the paper mentioned in the context information is \"Evaluation of chatgpt and bert-based models for turkish hate speech detection.\"",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        },
        {
            "query": "What is the file size of the PDF document mentioned in the context information?",
            "query_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            },
            "reference_contexts": [
                "Nur Bengisu \u00c7am and Arzucan \u00d6zg\u00fcr. Evaluation of chatgpt and bert-based models for\nturkish hate speech detection. In 2023 8th International Conference on Computer Science\nand Engineering (UBMK) , pp. 229\u2013233, 2023. doi: 10.1109/UBMK59864.2023.10286663.\n111"
            ],
            "reference_answer": "The file size of the PDF document mentioned in the context information is 1631422 bytes.",
            "reference_answer_by": {
                "model_name": "gpt-3.5-turbo",
                "type": "ai"
            }
        }
    ]
}